万字长文，对多模态LLM中对齐算法进行全面系统性回顾！
从现有
对齐算法涵盖的应用场景
，到
构建对齐数据集的核心因素
，再到
用于评估对齐算法的基准
，还有
对齐算法未来潜在发展方向
，全都梳理了一遍。
大语言模型（LLMs）能够通过简单的提示完成多种任务，且无需进行任务特定的训练。然而，这些模型主要处理文本数据，对于多模态数据的处理存在局限。
由于世界本质上是多模态的，包括视觉、听觉和文本等数据，研究者开始在LLM的基础上开发多模态大语言模型（MLLMs），以处理更复杂的数据形式。
然而，现有的MLLMs仍面临一系列挑战，尤其是在真实性、安全性、推理能力和与人类偏好对齐方面，这些问题尚未得到充分解决。
因此，针对这些问题的对齐算法应运而生，成为解决这些挑战的有效途径。
本文这项研究的主要贡献是对多模态大型语言模型（MLLMs）中的对齐算法进行全面的系统性回顾。
具体而言，探讨了以下四个关键问题：
这项研究由来自中国科学院自动化研究所、南京大学、中国科学技术大学、南洋理工大学、清华大学深圳国际研究生院、腾讯优图实验室、新加坡国立大学、理海大学、香港科技大学、松鼠Ai学习等机构的研究人员共同完成。
中国科学院院士谭铁牛、中国计算机学会会士王亮领衔。
以下是更多细节。
应用场景与代表性方法
应用场景
文章介绍了多模态大语言模型（MLLM）对齐算法的应用场景，分为三大层次：
通用图像理解与多模态o1
一般图像理解
MLLM对齐算法的初衷是解决多模态系统中的幻觉问题。最近的研究表明，这些算法不仅能提升幻觉的处理，还能增强模型的安全性、对话能力、推理能力等多个功能属性。
本节将系统地介绍几种创新的方法，按其主要应用场景分类：减少幻觉和提升其他能力。
减少幻觉
MLLM对齐算法的最初设计目的是减少幻觉现象。
例如，Fact-RLHF是第一个多模态的RLHF算法，使用了10K个人工标注的样本来训练奖励模型，并引入了每个token的KL惩罚、事实信息校准、以及正确性和长度惩罚等机制。
DDPO通过提高更正数据的权重，进一步优化了标准的DPO。
HA-DPO则利用MLLM生成图像描述，通过GPT-4验证幻觉并对正负样本进行重写，加入了辅助的因果语言建模损失以减少幻觉。
mDPO通过引入视觉损失函数来解决视觉信息忽视问题，并增加锚定机制来避免选定响应的概率下降。
提升综合能力
除了减少幻觉，一些算法还专注于提升模型的多方面能力。
例如，Silkie通过收集多样的指令数据集，并使用GPT-4V评估生成的响应，从而为应用DPO提供偏好数据。CLIP-DPO利用CLIP分数对数据进行标注，并应用DPO损失，从而同时提升幻觉减缓和零样本分类任务的表现。
SIMA通过让模型自我评估其生成的响应来构建偏好对，进一步提升了多图像任务的表现。
近期，MM-RLHF等方法通过更多样性的数据和算法，将alignment的效果进一步提升。
多模态o1发展
DeepSeek-R1的流行给MLLM社区带来了新的启示。
LMM-R1使用纯文本数学数据集，通过RLOO训练，并在多模态数学基准上取得了改进。
Open-R1-Video则利用GRPO方法提升了模型在视频领域的表现。
VLM-R1应用R1方法处理指代表达理解任务，进一步扩展了多模态推理的能力。
多图像、视频和音频
在这一部分，文章讨论了多图像、视频和音频任务中的挑战和解决方法。
扩展多模态应用
文章还介绍了在特定领域的扩展应用，提出了针对性更强的对齐方法。
作者分析多模态大语言模型的不同应用场景，详细介绍了多种算法和方法，涵盖了从通用图像理解到特定领域应用的各个方面。
主要贡献在于展示了如何通过优化对齐算法来减少幻觉现象并提升模型在不同任务中的综合能力，尤其在视频、音频、医学、数学等复杂领域的应用。
随着这些方法的不断优化，MLLM将在更多领域展现其强大的处理能力。
下表总结了目前alignment策略常见的损失函数形式：
MLLM对齐数据构造与现有数据总结
主要内容总结
在多模态大型语言模型（MLLM）的研究中，对齐数据集是关键组成部分。由于多模态数据集的构建涉及到大量的数据来源、生成方法和注释技术，研究者们对不同构建方法进行了分类。
这些数据集大致可以分为两类：引入外部知识的数据集和依赖自我标注的数据集。
通过这些分类，研究者可以更清晰地了解不同数据集的特点，进而为多模态系统的优化提供支持。
作者对现有MLLM对齐数据集进行了全面的分类与分析，详细介绍了不同构建方法的优缺点以及应用场景。研究主要关注以下几个方面：
通过这项工作，研究者们可以更加清楚地理解多模态数据集的构建策略，为未来的研究提供有力的支持。
引入外部知识的数据集
例如，LLaVA-RLHF通过人工选择正负响应收集了10k个样本，RLHF-V通过人工修正幻觉响应收集了1.4k个样本。
像LRV-Instruction通过GPT-4生成了400k个视觉指令，涵盖16个视觉语言任务。
例如，INTERACTIVECOT通过预定义分数构建了具身智能的偏好数据集。
自我标注的数据集
SQuBa使用微调后的模型生成负样本，并将其与正样本进行DPO对比。SymDPO通过将VQA/分类数据转化为ICL格式，以增强视觉学习。
Image DPO通过对图像进行扰动（如高斯模糊或像素化），而保持文本不变，构建DPO偏好对。
AdPO通过构建原始/对抗图像及其模型响应的偏好对，在优化过程中，两者的图像和文本内容在正负样本中有所不同。
实验发现
在实验部分，研究发现：
数据集规模与质量的平衡：
通过引入外部知识的数据集，能够提高数据的质量，但这也增加了构建成本。而自我标注的方法虽然能够大规模生成数据，但由于MLLM的性能限制，当前的自我标注数据集质量仍较低，且存在一定的分布偏移问题。
自动化增强的潜力：
随着自动化数据增强技术的发展，未来的自我标注方法将可能解决当前数据质量低的问题，并提高数据的多样性和可信度。
总的来说，数据集的构建方法和质量控制是影响MLLM对齐效果的关键因素，未来的研究应关注如何在保证数据质量的同时，降低成本并提高数据集的规模。
模型评估
现有的MLLM对齐评估基准被分为六个关键维度：
通用知识
（评估基础能力）、
幻觉
（衡量生成内容与事实的一致性）、
安全性
（评估响应中降低风险的能力）、
对话
（测试模型是否能输出用户要求的内容）、
奖励模型
（评估奖励模型的表现）和
与人类偏好的对齐
。
通用知识
大多数基准优先考虑高质量、人工注释的数据集，这些数据集专门为实际应用场景量身定制。
例如，MME-RealWorld包含来自13K张图像的29K个问答对，MMMU包含来自学术来源的11.5K个问题。MMStar通过减少数据泄漏和强调视觉依赖性来增强可靠性。
许多基准引入了创新方法，如MMBench的双语评估与CircularEval，MMT-Bench的任务图用于域内外分析，以及BLINK专注于视觉感知任务。这些框架提升了评估精度，揭示了模型的局限性。
任务通常需要高级的多模态推理能力，例如MathVista的数学视觉整合、SQA3D的3D情境问答以及MMMU对图表和地图的覆盖。
这些基准推动模型解决跨学科的挑战，通过策划具有挑战性的、细粒度的任务（如MVBench中的时间理解、Mantis-Instruct中的多图像处理），旨在提升模型解决现实世界问题的能力，特别是在细致感知和推理方面。
幻觉
这些基准系统地识别并分类多模态模型中的幻觉问题，包括对象幻觉（Object HalBench）、内在和外在幻觉（VideoHallucer）以及关联偏差（VALOR-Eval）。它们强调在视觉、文本和序列上下文中的细粒度评估。
许多基准提出了创新的框架，例如基于投票的查询（POPE）、LLM驱动的评分（HaELM、RefoMB）、开放词汇检测（OpenCHAIR）、无注释评估（GAVIE）、无LLM管道（AMBER）和GPT-4辅助的推理分析（Mementos）。
这些方法强调自动化、可扩展的评估，同时解决数据泄漏和语言先验等问题。
数据集优先采用细粒度的人类注释（M-HalDetect、HallusionBench）和合成数据生成（VHTest、MHaluBench），它们平衡了现实世界的复杂性（PhD的反常识图像、ActivityNet-QA的58K问答对）和受控挑战（R-Bench的鲁棒性分析）。
一些基准专注于特定任务，如多语言支持（MHumanEval），而另一些则处理更广泛的问题，如偏见和干扰（Bingo）。所有这些都旨在提高模型在实际场景中的鲁棒性。
通过提出对齐策略（如RLAIF-V的开源反馈）并提出统一框架（HQH），这些基准为开发更可靠的多模态系统提供了指导。
安全性
一些研究引入了新颖的技术，例如基于扩散的对抗性攻击（AdvDiffVLM）、红队框架（RTVLM）和后期微调策略（VLGuard）。
这些方法通过模拟现实世界威胁或提升模型的抗干扰能力来增强评估的严谨性。
像MultiTrust和RTVLM这样的基准通过多个维度统一了可信度评估（如真实性、公平性），而另一些基准则专注于特定挑战，如超出分布（OOD）泛化（VLLM-safety-bench）或过度敏感性（MOSSBench）。这些基准为模型的限制提供了整体的见解。
MM-RLHF-SafetyBench从现有数据集中进行采样，进一步涵盖了如对抗性攻击、隐私、红队攻击和有害内容检测等领域。
对话
这些基准优先评估基础的视觉技能，如低级感知能力（Q-Bench、LLVisionQA）、低级信息的描述能力（LLDescribe）和质量评估。
它们强调模型解释和表达细粒度视觉信息的能力。
几个基准测试了模型在具有挑战性场景中的泛化能力，包括非常规图像（LLaVA Bench-Wilder）、跨领域任务（LiveBench的数学/新闻整合）和对抗性提示（Vibe-Eval的高难度问题）。这些基准揭示了模型在标准数据集之外的适应能力。
奖励模型
每个基准都针对特定的评估维度，例如多语言能力（M-RewardBench中的23种语言）、对齐/安全性/偏见（MJ-Bench）、通过人类注释增强可解释性和最终模型评分能力（MM-RLHF-RewardBench）以及MLLM在辅助评判者跨多种模态中的能力（MLLM-as-a-Judge的评分与成对比较）。
这些框架揭示了模型在结构化和OOD场景中的优缺点。
高质量的数据集通过人类与AI合作（如VL-RewardBench的注释管道）或结构化三元组设计（RewardBench）策划而成，任务从简单的偏好排序到复杂的推理，推动模型处理如幻觉和伦理对齐等细致挑战。
对齐
一些基准研究了模型与人类偏好的对齐能力。
Arena-Hard是一个全面的多维度基准，旨在评估中文LLM的对齐能力。AlpacaEval-V2提出了一种简单的回归分析方法，用于控制自评中的长度偏差。Arena-Hard通过增加模型性能的分离度三倍，达到了98.6%的与人类偏好排名的相关性。MM-AlignBench是一个专门设计的手工注释基准，旨在评估与人类价值观的对齐。
总体而言，许多当前的MLLM对齐算法侧重于防止模型生成幻觉，同时探索如何利用对齐算法提升MLLM的通用知识和对话能力，这是未来的重要方向。
一些研究者将不安全的响应视为与人类偏好不对齐，因此应用MLLM对齐算法来解决安全问题。这些框架中奖励模型的有效性，尤其是在引导对齐方面的表现，值得进一步研究。
此外，针对与人类偏好的对齐，基准也从LLM领域发展到了MLLM领域。
未来工作与挑战
随着多模态大型语言模型（MLLM）的迅速发展，将它们与人类偏好对齐已经成为研究的重点。然而，仍然存在若干挑战。
首先，高质量和多样化数据集的稀缺问题仍然未得到有效解决。其次，许多方法未能有效利用视觉信息，往往主要依赖文本来构建正负样本，忽略了多模态数据的全部潜力。此外，缺乏全面的评估标准，当前的方法通常仅在幻觉或对话任务等特定类型的基准上进行验证，这使得它们的普适性难以评估。
通过借鉴LLM后期训练策略和智能体研究的进展，可以揭示现有MLLM对齐方法中的局限性。克服这些挑战对于开发更强大和全面的对齐方法至关重要。
数据挑战
MLLM对齐面临两个关键的数据相关挑战：数据质量和覆盖范围。
首先，高质量的MLLM对齐数据的可用性有限。与LLM相比，获取和注释多模态数据要复杂得多，因为它涉及多个模态的处理。
其次，现有数据集在涵盖多样化多模态任务方面存在不足，例如光学字符识别、数学问题和图表理解等任务。构建一个涵盖广泛任务的综合数据集是一项极具挑战的工作。
据作者所知，目前没有一个公开的、完全人工注释的多模态数据集样本量超过200,000个。
这些在数据质量和覆盖范围方面的限制，成为有效对齐MLLM的重大障碍。
利用视觉信息进行对齐
目前的对齐数据可以表示为：偏好数据D=(x, I, yw, yl)，其中x是问题，I是图像，yw、yl分别表示正确和错误的响应。
在当前的研究中，主要有三种方法用于利用视觉信息来增强对齐性能，但每种方法都有其局限性：
研究人员创建新的图像Ineg，并使用(yw｜X, Ineg)作为负样本。此方法通过减少幻觉和提高MLLM对不同图像的鲁棒性来改善对齐效果。然而，视觉负样本通常依赖于扩散算法或图像修改，这些方法缺乏强有力的质量度量，并且计算成本较高。
在这种方法中，研究人员创建一个新的图像Ineg，使用该图像生成额外的响应yneg，然后将(yneg｜X, I)视为负样本。此方法增加了文本比较的多样性，但生成额外负样本的过程增加了计算开销。
该方法通过计算文本与图像之间的相似度分数来筛选数据，或将其作为强化学习奖励函数的一部分。虽然这种方法有助于减少数据噪声，但评分的质量依赖于评估模型的质量，可能受到模型偏见的影响。
每种方法在利用视觉数据增强MLLM对齐方面都有一定的作用，但在效率、成本和潜在偏见方面存在权衡。
综合评估
大多数MLLM对齐研究主要评估其算法在幻觉、对话能力或安全性等几个关键领域的表现。
然而，未来的研究应采用更全面的评估方法，跨更广泛的任务评估对齐方法，以更好地展示其普适性和有效性。
全模态对齐
Align-anything开创了通过多模态数据集“align-anything-200k”实现全模态对齐的研究，涵盖了文本、图像、音频和视频。这项研究展示了不同模态之间的互补效应。
然而，他们的工作仍处于初期阶段，每种模态的数据集相对较小，限制了其任务覆盖范围。
此外，提出的算法仅是DPO方法的初步改进，未能充分利用每种模态固有的独特结构信息。
未来，超越图像/文本领域的对齐算法设计，尤其是针对其他模态的对齐，将是一个关键的趋势。
MLLM推理
最近，由OpenAI（o1）和DeepSeek-R1代表的推理LLM已经证明，强化学习算法和偏好数据对于提高LLM在复杂问题求解、长时上下文理解和生成任务中的表现至关重要。
本文将探讨从LLM推理增强研究中获得的洞察及其对对齐MLLM的影响，主要从数据和优化框架两个维度进行分析。
(1) 数据。
目前，用于推理增强的数据集普遍达到百万样本规模（如Qwen-2.5-MATH）。
(2) 优化框架。
此外，Mini-Max采用了离线+在线采样策略，进一步提升了模型性能。
例如，Llama 3包含了六轮DPO迭代，而DeepSeek通过温度变化的采样和反射/验证提示来优化推理深度（长链式推理）和简洁性。
一个是去除评估模型并用稀疏奖励训练策略，从而减少一半的参数量（如DPO和GRPO）；另一个是精炼评估模型的设计，例如引入比率作为优势函数的PRIME和通过重塑正负样本奖励的OREAL。
通过优先考虑高质量数据和创新的优化框架，MLLM领域正朝着更有效、可扩展的模型发展，这些模型也能更好地解锁MLLM的推理潜力。
LLM对齐的启示
对LLM的对齐已经成为最近研究的一个关键焦点，提供了许多有价值的见解，可以为MLLM的开发提供指导。通过审视现有LLM对齐策略的经验教训，可以揭示出有助于提升MLLM研究的关键原则：
(1) 提高训练效率。
当前MLLM的对齐方法依赖于DPO损失函数。然而，由于DPO需要同时加载策略模型和参考模型，训练速度会显著下降。是否可以利用类似SimPO的无参考方法进一步提升训练效率？
这一方法可能加速训练过程，同时减少对参考模型的依赖。进一步研究参考模型在MLLM对齐中的具体作用和影响，对于提高效率和优化模型设计至关重要。
(2) 减轻过度优化/奖励黑客问题。
在使用DPO或RLHF进行LLM对齐时，过度优化仍然是一个关键挑战，即通过学习到的代理奖励模型来提高性能，但真正的质量可能停滞或退化。
为应对这一挑战，解决策略包括：
MLLM作为智能体
MLLM结合了LLM强大的推理能力和处理来自多种模态（如图像、文本和音频）数据的能力。这使得它们能够从多种信息源中提取知识并进行综合分析，在处理复杂的现实任务中具有很大优势。
然而，要将MLLM转变为高效的智能体，仍然需要解决几个待处理的问题。
论文链接：https://arxiv.org/pdf/2503.14504
GitHub链接：https://github.com/BradyFU/Awesome-Multimodal-Large-Language-Models/tree/Alignment