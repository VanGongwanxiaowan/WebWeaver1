# Abstract 摘要
This paper tackles open-ended deep research (OEDR), a complex challenge where AI agents must synthesize vast web-scale information into insightful reports. Current approaches are plagued by dual-fold limitations: static research pipelines that decouple planning from evidence acquisition and monolithic generation paradigms that include redundant, irrelevant evidence, suffering from hallucination issues and low citation accuracy. To address these challenges, we introduce WebWeaver, a novel dual-agent framework that emulates the human research process. The planner operates in a dynamic cycle, iteratively interleaving evidence acquisition with outline optimization to produce a comprehensive, citation-grounded outline linking to a memory bank of evidence. The writer then executes a hierarchical retrieval and writing process, composing the report section by section. By performing targeted retrieval of only the necessary evidence from the memory bank via citations for each part, it effectively mitigates long-context issues and citation hallucinations. Our framework establishes a new state-of-the-art across major OEDR benchmarks, including DeepResearch Bench, DeepConsult, and DeepResearchGym. These results validate our human-centric, iterative methodology, demonstrating that adaptive planning and focused synthesis are crucial for producing comprehensive, trusted, and well-structured reports.

本文探讨了开放式深度研究（OEDR）这一复杂挑战，在该挑战中，人工智能智能体必须将海量网络规模的信息整合为富有洞察力的报告。当前方法存在两方面的局限性：静态研究流程将规划与证据获取割裂开来；整体生成模式包含冗余、无关的证据，存在幻觉问题且引用准确性低。为应对这些挑战，我们提出了WebWeaver，这是一种模拟人类研究过程的新型双智能体框架。规划器以动态循环方式运作，通过迭代交错进行证据获取和大纲优化，生成一个全面的、基于引用的大纲，该大纲链接到证据记忆库。随后，撰写器执行分层检索和写作过程，逐节撰写报告。通过针对每个部分，借助引用从记忆库中定向检索仅需的证据，它有效缓解了长上下文问题和引用幻觉。我们的框架在包括DeepResearch Bench、DeepConsult和DeepResearchGym在内的主要OEDR基准测试中建立了新的最先进水平。这些结果验证了我们以人为本的迭代方法，表明自适应规划和聚焦综合对于生成全面、可信且结构良好的报告至关重要。


# 1Introduction 1 引言

Large Language Models (LLMs) (OpenAI, 2025b; Qwen Team, 2025; Liu et al., 2024; DeepMind, 2025; anthropic, 2025) have demonstrated remarkable capabilities across a wide array of well-defined tasks, from factual question answering (Wei et al., 2025; Mialon et al., 2023) to document summarization (Zhang et al., 2025) and code generation (Jiang et al., 2024). Their success, however, has largely been confined to scenarios with clear instructions and ground-truth answers. The true frontier for autonomous AI lies in transcending these structured problems to tackle the complex, open-ended challenges that define human-level knowledge work—a process driven by curiosity, synthesis, and the discovery of novel insights. We term this challenge open-ended deep research (OEDR). Unlike tasks with ground-truth answers, OEDR requires an agent to independently navigate and digest a vast corpus of information, often exceeding 100 web pages and PDFs, to form a detailed report with accurate citations. This represents a monumental challenge, and as shown in Fig. 2, 2, most proprietary agents fail dramatically on report quality (RACE) and citation accuracy (FACT) of DeepResearch Bench (Du et al., 2025), highlighting a critical gap we aim to address.

大型语言模型（LLMs）（OpenAI，2025b；通义千问团队，2025；Liu等人，2024；DeepMind，2025；Anthropic，2025）在众多定义明确的任务中展现出了卓越的能力，从事实问答（Wei等人，2025；Mialon等人，2023）到文档摘要（Zhang等人，2025）以及代码生成（Jiang等人，2024）。然而，它们的成功在很大程度上局限于具有清晰指令和真实答案的场景。自主人工智能的真正前沿在于超越这些结构化问题，去应对那些定义人类级知识工作的复杂、开放式挑战——这是一个由好奇心、综合能力和新见解发现驱动的过程。我们将这一挑战称为开放式深度研究（OEDR）。与有真实答案的任务不同，OEDR要求智能体独立浏览和消化海量信息（通常超过100个网页和PDF），以形成带有准确引用的详细报告。这是一项巨大的挑战，如图2、2所示，大多数专有智能体在深度研究基准（DeepResearch Bench）（Du等人，2025）的报告质量（RACE）和引用准确性（FACT）方面表现极差，这凸显了我们旨在解决的一个关键差距。

Current attempts to tackle OEDR fall into two main categories: proprietary and open-source solutions. While several powerful proprietary agents show strong performance (OpenAI, 2025a; Research, 2025b; d; a), their prohibitively expensive APIs and restrictive quotas create significant barriers, limiting widespread adoption and hindering academic research. Consequently, the focus has shifted towards open-source alternatives, which predominantly follow two paradigms. As shown in Fig. 3, the first is a straightforward "search-then-generate" approach (Tao et al., 2025; Roucher et al., 2025; Li et al., 2025a), where the agent gathers all information before directly generating a report. This method results in low-quality, incoherent outputs because it lacks an outline to guide writing. The second, more sophisticated approach either initializes a static outline to guide searching (Han et al., 2025; Research, 2025e; c) or searches information for outlining (Shao et al., 2024; Xiong et al., 2025). However, these strategies are critically flawed: the upfront outline often suffers from the LLM’s internal outdated knowledge. The search-first method also constrains the searching boundary without outline guidance. Furthermore, feeding all searched materials or redundant evidence into the context for final writing is susceptible to the “loss in the middle” issue (Liu et al., 2023) and increased hallucinations, compromising the report’s accuracy and depth (Bai et al., 2024; Wu et al., 2025c).

目前解决OEDR的尝试主要分为两类：专有解决方案和开源解决方案。虽然一些强大的专有智能体表现出优异的性能（OpenAI，2025a；Research，2025b；d；a），但它们昂贵的API以及限制性的配额设置了重大障碍，限制了其广泛应用并阻碍了学术研究。因此，焦点已转向开源替代方案，这些方案主要遵循两种范式。如图3所示，第一种是简单直接的“先搜索后生成”方法（Tao等人，2025；Roucher等人，2025；Li等人，2025a），即智能体先收集所有信息，然后直接生成报告。这种方法会导致输出质量低、内容不连贯，因为它缺乏指导写作的大纲。第二种更复杂的方法要么初始化一个静态大纲来指导搜索（Han等人，2025；Research，2025e；c），要么为制定大纲而搜索信息（Shao等人，2024；Xiong等人，2025）。然而，这些策略存在严重缺陷：预先制定的大纲往往受限于大语言模型内部过时的知识。先搜索的方法在没有大纲指导的情况下也会限制搜索范围。此外，将所有搜索到的材料或冗余证据放入上下文进行最终写作，容易出现“中间丢失”问题（Liu等人，2023），并增加幻觉现象，从而影响报告的准确性和深度（Bai等人，2024；Wu等人，2025c）。

Figure 3:(a) the search-then-generate paradigm first gathers information and then directly generates a report; (b) the paradigms decouple the searching and outline generation; (c) WebWeaver not only enables a dynamic research cycle where the outline and search strategy co-evolve but allows hierarchical and attentional writing by retrieving only relevant evidence with citations in the outline.
图3：（a）“先搜索后生成”范式首先收集信息，然后直接生成报告；（b）这些范式将搜索和大纲生成分离开来；（c）WebWeaver不仅实现了大纲和搜索策略共同演进的动态研究循环，还通过在大纲中检索仅包含引用的相关证据，实现了分层且具有注意力的写作。


The key, we believe, lies in abandoning rigid, machine-like pipelines and instead embracing the organic process of human intellect. Our approach is designed to do just that: it teaches the agent to research like a person. A human expert doesn’t decouple and fix their drafting and searching phases; they allow these two phases to co-evolve until converging to a comprehensive outline. We implement this principle through an agentic loop where actions of searching and outline optimization interleave. As the agent explores the web-scale information landscape, its discoveries continuously inform and reshape the outline. Critically, this refined outline then acts as a strategic blueprint, actively guiding subsequent searches to fill identified knowledge gaps and explore underdeveloped sections. This creates a true feedback loop where outlining and discovery co-evolve. Then, when it is time to write, our agent should accept only the relevant context. Just as a human writer would refer to specific notes for a specific chapter, our agent composes each section by focusing only on the source-grounded materials. By doing so, it operates with clarity and precision, crafting a final report that is not just a summary of data but well-structured and insightful pieces of analysis with accurate citations.
我们认为，关键在于摒弃僵化的、机器般的流程，转而采用人类智力所特有的有机过程。我们的方法正是为此而设计：它教会智能体像人一样进行研究。人类专家不会将起草和搜索阶段割裂开来并固定不变，而是让这两个阶段共同发展，直至形成一个全面的大纲。我们通过一个智能体循环来实现这一原则，在这个循环中，搜索和大纲优化的行动相互交织。当智能体在网络级的信息环境中探索时，它的发现会不断为大纲提供信息并重塑大纲。至关重要的是，这个经过完善的大纲随后会成为一个战略蓝图，积极指导后续的搜索，以填补已识别的知识空白并探索有待深入的部分。这就形成了一个真正的反馈循环，在其中，大纲制定和信息发现共同发展。然后，到了写作的时候，我们的智能体会只接纳相关的上下文。就像人类作者会为特定章节参考特定笔记一样，我们的智能体在撰写每个部分时，也只专注于有来源依据的材料。通过这种方式，它能够清晰而精准地运作，最终写出的报告不仅是数据的汇总，更是结构严谨、富有洞见的分析，并且带有准确的引用。

To this end, we propose WebWeaver by following the human-centric philosophy, a dual-agent framework comprising a planner and a writer. As shown in Fig. 3, the planner embodies the exploratory research phase, operating in a dynamic, agentic cycle that iteratively interleaves evidence acquisition with outline optimization, culminating in a comprehensive, source-grounded research outline, where each section is explicitly linked via citations to a curated memory bank of source evidence. When it turns to the writing phase, to address the critical long context and attentional context management challenge, the writer executes a memory-grounded, citation-driven, hierarchical synthesis process. It constructs the report section by section, performing targeted retrieval of only the relevant evidence from a structured memory bank via citations in the outline for each subtask. This synergistic division of labor enables our agent to navigate complex information landscapes and produce reports that are both comprehensive in scope and trusted in their evidentiary grounding.
为此，我们遵循以人为本的理念，提出了WebWeaver，这是一个由规划器和撰写器组成的双智能体框架。如图3所示，规划器体现了探索性研究阶段，它在一个动态的、具有智能体特性的循环中运行，迭代地将证据获取与大纲优化交织在一起，最终形成一个全面的、以来源为基础的研究大纲，其中每个部分都通过引文明确链接到精心整理的来源证据记忆库。在撰写阶段，为了应对关键的长上下文和注意力上下文管理挑战，撰写器执行一个以记忆为基础、以引文为驱动的分层合成过程。它逐节构建报告，通过大纲中针对每个子任务的引文，从结构化记忆库中定向检索仅相关的证据。这种协同分工使我们的智能体能够驾驭复杂的信息环境，并生成范围全面且证据基础可信的报告。

Extensive experiments demonstrate that WebWeaver achieves state-of-the-art (SOTA) performance and outperforms both the proprietary and open-source agent systems on three recent and challenging open-ended deep research benchmarks. Detailed discussion is produced to demonstrate the effectiveness of outline optimization and memory-grounded synthesis. Critically, WebWeaver enables agentic finetuning of small models for practical use. We construct a high-quality SFT dataset, WebWeaver-3k, generated by our framework. The experiments with WebWeaver-3k demonstrate that the complex skills of thinking, searching, and writing can be learned, enabling smaller, accessible models to achieve the expert-level performance previously confined to large-scale proprietary systems.
大量实验表明，WebWeaver 达到了最先进（SOTA）的性能，在三个最新且具有挑战性的开放式深度研究基准上，其表现优于专有和开源的智能体系统。本文进行了详细讨论，以证明大纲优化和基于记忆的合成的有效性。关键是，WebWeaver 支持对小型模型进行智能体微调，以便实际使用。我们构建了一个高质量的监督微调（SFT）数据集 WebWeaver-3k，该数据集由我们的框架生成。基于 WebWeaver-3k 的实验表明，思考、搜索和写作等复杂技能是可以习得的，这使得更小、更易获取的模型能够达到以前仅限于大规模专有系统的专家级性能。

# 2Preliminaries 2 预备知识

Problem definition. We consider the open-ended research question without the ground-truth answers. Given an open-ended question, the agents need to search relevant information and finally output a report or article. To achieve this, we implement a planner for collecting information, a memory to store materials, and a writer for report generation. For both the planner and writer, we adopt ReAct (Yao et al., 2023) as the agent’s framework. Upon receiving a question, they perform several iterations of thought-action-observation. Specifically, in each iteration, based on the existing context, the LLM generates a thought and executes a parsable action, then awaits the environment to return an observation. The planning and writing stages terminate with the output token of “<terminate>”. A complete trajectory with 
T
 iterations can be defined as

问题定义。我们考虑的是没有真实答案的开放式研究问题。给定一个开放式问题，智能体需要搜索相关信息并最终输出一份报告或文章。为实现这一目标，我们设计了一个用于收集信息的规划器、一个用于存储材料的存储器以及一个用于生成报告的撰写器。对于规划器和撰写器，我们均采用ReAct（Yao等人，2023）作为智能体框架。在接收到问题后，它们会执行多轮“思考-行动-观察”循环。具体而言，在每一轮循环中，大语言模型会基于现有上下文生成一个思考结果并执行一个可解析的行动，然后等待环境返回观察结果。规划和撰写阶段会以输出 token“<terminate>”结束。一个包含
T
轮循环的完整轨迹可以定义为

	
ℋ
T
=
(
τ
0
,
a
0
,
o
0
,
…
,
τ
i
,
a
i
,
o
i
,
…
,
τ
T
,
a
T
)
,
Actions. For the planner, the action space consists of search, write outline, and terminate. Given the search queries, the search engine returns titles, snippets, and corresponding URLs. To save context space, we further execute the actions of URL selection, parsing pages via URLs, summarizing relevant contents, and extracting evidence with LLMs following the searching queries. The search tool finally returns the selected URLs with their corresponding summaries and evidence. The action of “write outline” is to generate and optimize the outline with citations linking to the evidence in the memory bank, and the “terminate” action is to terminate the planning process.
行动。对于规划者而言，行动空间包括搜索、撰写大纲和终止。给定搜索查询后，搜索引擎会返回标题、片段及相应的URL。为节省上下文空间，我们会进一步执行URL选择、通过URL解析页面、总结相关内容以及根据搜索查询利用大语言模型提取证据等操作。搜索工具最终会返回选定的URL及其相应的摘要和证据。“撰写大纲”这一行动旨在生成并优化大纲，其中包含链接到记忆库中证据的引用，而“终止”行动则是结束规划过程。

For the writer, the action space consists of retrieve, write, and terminate. Besides the terminate action, the retrieve action is to retrieve evidence from the memory bank by providing the grounded citations in the outline. The write action is provided to write the section of the report.
对于作者而言，动作空间包括检索、写作和终止。除了终止动作外，检索动作是通过在大纲中提供有根据的引用从记忆库中检索证据。写作动作用于撰写报告的章节。

Memory bank. Answering an open-ended question requires long-context input of the collected information and long-context output of the final report. To search sufficient materials, the planner often searches and parses more than 100 web pages, with more than 100k tokens. The writer often outputs more than 20k tokens to produce a comprehensive report. Prior open-sourced deep research agents (Roucher et al., 2025; Research, 2025e; c) include all the raw materials (e.g., web pages and PDF files) in the LLM context, leading to quality degradation due to attentional failures like the “lost in the middle” problem, poor coherence, and increased hallucinations (Liu et al., 2023; Li et al., 2024a; Bai et al., 2024; Wu et al., 2025c). To this end, we introduce a memory to achieve context management for both planner and writer. Only a short summary of the web page or PDF file is included in the search context, and only necessary raw pages will be retrieved from the memory to write the corresponding sections via the citations in the outline.
记忆库。回答一个开放式问题需要收集到的信息作为长上下文输入，以及最终报告作为长上下文输出。为了搜索到足够的材料，规划者通常会搜索和解析100多个网页，涉及超过10万个标记。撰写者通常会输出超过2万个标记来生成一份全面的报告。现有的开源深度研究智能体（Roucher等人，2025；Research，2025e；c）会将所有原始材料（如网页和PDF文件）都包含在大语言模型的上下文中，这会由于注意力失效（如“中间遗忘”问题）、连贯性差以及幻觉增多而导致质量下降（Liu等人，2023；Li等人，2024a；Bai等人，2024；Wu等人，2025c）。为此，我们引入了一种记忆机制，为规划者和撰写者实现上下文管理。搜索上下文中只包含网页或PDF文件的简短摘要，并且只有必要的原始页面会通过大纲中的引用从记忆中检索出来，用于撰写相应的部分。



